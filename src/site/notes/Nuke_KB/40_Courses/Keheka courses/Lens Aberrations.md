---
{"dg-publish":true,"permalink":"/nuke-kb/40-courses/keheka-courses/lens-aberrations/"}
---

# Refraction Of Light

Light rays don’t always travel in straight lines.

They break and bend when they leave one medium (such as air) and go into another medium (like water or glass). This redirection is known as _refraction of light_.

It’s the reason why photographic lenses in their simplest design have so many problems, and why more complex lenses are built to correct for those issues.

[![](https://www.keheka.com/content/images/2023/08/Refraction_A.png)](https://www.keheka.com/content/images/2023/08/Refraction_A.png)

Refraction causes a ray of light to be redirected by a clear plastic block.

[![](https://www.keheka.com/content/images/2023/08/Refraction_B.jpg)](https://www.keheka.com/content/images/2023/08/Refraction_B.jpg)

Refraction causes a change in the apparent shape and position of a straw in a glass of water.

> [!important]
> 
> By controlling the shape of the medium which the light travels through, we can control the direction of the light.

[![](https://www.keheka.com/content/images/2023/08/Refraction_Concave.jpg)](https://www.keheka.com/content/images/2023/08/Refraction_Concave.jpg)

A **concave** lens spreads the parallel light rays outward. And vice versa, it straightens out light rays that are angled inward.

> [!important]
> 
> Also note that some light is lost along the way due to reflection. I.e. the faint, angular light rays on the screen left side of the lens.

[![](https://www.keheka.com/content/images/2023/08/Refraction_Convex.jpg)](https://www.keheka.com/content/images/2023/08/Refraction_Convex.jpg)

A **convex** lens focuses the parallel light rays inward. And vice versa, it straightens out light rays that are angled outward.

> [!important]
> 
> An easy way to remember how to distinguish between the two, is that a con**cave** lens looks **cave**d in, while a conv**ex** lens looks fl**ex**ed.

Technically, you could use a single, convex piece of glass as the lens for taking a photo. (The photo would be upside down and not evenly focused, though!).

But in real life scenarios, the light from the scene will not come from a single direction and arrive at the camera lens in neat, parallel rays. Rather, it will enter the lens from  
multiple angles.

The curvature of the lens also makes the light rays refract differently depending on where they enter the lens (see the _Spherical Aberration_ section).

And so in practice, it becomes difficult to focus the light rays to a point and capture a sharp image using a single lens. However, _several_ lenses put together can achieve this. Which is why lens manufacturers layer multiple pieces of glass inside of a lens body.

Let's take a look.

# The Anatomy Of A Camera Lens

The way a camera lens is built directly affects how light is captured in the image.

[![](https://www.keheka.com/content/images/2023/08/Lens_Cut.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Cut.jpg)

[![](https://www.keheka.com/content/images/2023/08/LensAnatomy.jpg)](https://www.keheka.com/content/images/2023/08/LensAnatomy.jpg)

The anatomy of a camera lens.

Inside a lens there are a number of precisely machined convex and concave glass elements. They are all arranged in a highly specific and calibrated way.

Their job is to focus the light that’s entering the camera lens onto the sensor that’s inside of the camera body.

Here is a video that explains it nicely:

> [!info] How Lenses Function  
> Revisit the physics of how lenses work, and how refraction, spherical aberration, and chromatic aberration come about.  
> [https://youtu.be/EL9J3Km6wxI](https://youtu.be/EL9J3Km6wxI)  

How lenses function.

  

In a perfect lens, the light hitting the camera's sensor would be an exact representation of the scene.

In a real lens, however, the different wavelengths of light refracting through the lens will not perfectly converge at the same focal point.

> [!important]
> 
> To converge just means to gather together. And the focal point is the point where the light rays meet and come into focus.

The reason the light rays don't perfectly converge is because:

# Lenses Are Imperfect

Despite using extremely precise machinery, camera and lens manufacturers are subject to real world physical limitations.

They cannot create mathematically perfect equipment like our CG cameras and lenses.

[![](https://www.keheka.com/content/images/2023/08/Clean_Lens.jpg)](https://www.keheka.com/content/images/2023/08/Clean_Lens.jpg)

Even a seemingly perfect, clean camera lens can’t perfectly guide the light from the scene to the camera sensor:

- Real camera lenses have microscopic imperfections. And after some use –  
    dust, dirt, water spots, or scratches may accumulate on the lens or  
    inside of the lens body.

- The curvature of the  
    glass also means that light gets bent more as it passes through the  
    outer edges of the lens than through the centre, creating distortions in the image.

- In real life, the light rays are  
    split on their way through the lens. Which means they hit the sensor in  
    different places. They scatter and reflect, creating aberrations and  
    artefacts.

- Lenses often don’t stop unwanted  
    light from entering and reaching the sensor, either. In addition to the  
    actual image-forming light, non-image-forming light such as veiling  
    glares may be captured in the image.

- Adjusting  
    the physical mechanisms in a lens also creates interesting phenomena,  
    which affect the image in unexpected ways. For example, when pulling  
    focus the focal length tends to change.

As you can see, there is a lot going on in the process of capturing images. We'll look at all of these lens effects, and more, in detail later on in the guide.

But first, to help understand them, it’s useful to learn about the concept of a circle of confusion.

# Circle Of Confusion

A number of aberrations that appear in an image, such as focus issues and colour fringing, stem from the same core issue:

The camera lens is unable to perfectly focus/converge light rays from the scene at the sensor.

When a ray from a point of light in the scene reaches the lens, and then converges at the sensor, it will be sharp.

However, if it converges before or after the sensor there will be a wider spread of the light on the sensor. Instead of a sharp point, a larger spot of light (shaped like the aperture) will hit the sensor.

[![](https://www.keheka.com/content/images/2023/08/Circles_Of_Confusion.jpg)](https://www.keheka.com/content/images/2023/08/Circles_Of_Confusion.jpg)

Circles of confusion.

  

> [!important]
> 
> Even at the sharpest focus, a point of light will still technically be imaged as a spot rather than a point. Also see the _Spherical Aberration_ section further below.

In the diagram above, we shoot a point of light at three positions:

1. Closer to the lens than the focal plane.

1. Exactly at the focal plane.

1. Further away from the lens than the focal plane.

  

> [!important]
> 
> The focal plane is a plane that is perpendicular to the axis of the lens, on which objects in the scene appear in focus. (It's actually a slightly curved focal _surface_ rather than a plane due to the curved lens elements – see the _Field Curvature_ section later on in the article – but for simplicity we use a plane).

  

> [!important]
> 
> In camera terminology, there are two things that are _both_ called the focal plane: the front and the rear/back focal plane. In this case, we're talking about positioning the point of light at the focal plane that’s **in front of** the lens, in the actual scene. The thin area the sensor occupies, which is behind the lens inside of the camera body, is also called a focal plane – or image plane/sensor plane/film plane. (And because the sensor is usually flat instead of slightly curved like the lens elements, we get some issues at the edges of the frame, which we will look into).

[![](https://www.keheka.com/content/images/2023/08/Sensor_Plane.png)](https://www.keheka.com/content/images/2023/08/Sensor_Plane.png)

The sensor's position inside of the camera body is _also_ called a focal plane, and is often marked on cameras with the symbol above.

Let's go back to the diagram. Light rays travel from the point to the lens, where they are bent inward toward the sensor (which is represented by the vertical dark lines on the right).

At each position of the point of light, different things happen:

1. The rays coming from the point of light that's in front of the focal plane _would_ meet at a point **behind the sensor** (stippled yellow lines). But instead they hit the sensor in a wider  
    area than just a sharp point – producing a spot. The result is a  
    defocused image of the point of light.

1. The rays coming from the point of light that's on the focal plane converge at a point **exactly on the sensor**, resulting in a sharp image of the point of light.

1. The rays coming from the point of light that's behind the focal plane converge at a point **in front of the sensor**. Which means they continue on, cross each other, and hit the sensor in a wider area than just a sharp point – producing a spot. The result is  
    again a defocused image of the point of light.

### Not Just One Point Of Light

Imagine now that instead of a point of light, you have a person  
standing in front of the camera. An enormous number of light rays from  
the scene will bounce off the person and travel to the lens.

Depending on where the person is standing in relation to the focal plane, the rays that make it through the lens will end up converging at, in front of, or behind, the sensor. Which determines how sharp the person, or the different parts of the person, will be.

That said, there is some wiggle room when it comes to the focal plane.

The image will _technically_ only be 100% sharp at the focal plane. However, there is a range in front of and behind the focal plane which will be acceptably in focus. As in, it will look sharp to our eyes.

> [!important]
> 
> The circle of confusion can be larger than a point, as long as it is indistinguishable from a point to our eyes.

**Spoiler alert**: This is all related to depth of field and bokeh.

### On The Fringes

The circle of confusion also drives other lens aberrations.

Light has different wavelengths. We see these different wavelengths as different colours. You can observe this when looking at the rainbow, where white sunlight (which contains all of the colours/wavelengths) is split into its component colours.

[![](https://www.keheka.com/content/images/2023/08/Rainbow.jpg)](https://www.keheka.com/content/images/2023/08/Rainbow.jpg)

A rainbow is a separation of the different wavelengths that make up white sunlight.

  

> [!important]
> 
> The colours of the rainbow are actually a completely smooth, continuous spectrum. From the longest wavelengths at the outer edge, to the shortest wavelengths at the inner edge. The colour bands we see are just the result of our limited human colour vision.

Of the colours in the rainbow, red has the longest wavelength and violet has the shortest wavelength. The colours going from red to violet have gradually shorter wavelengths.

Let’s look at it in terms of the **R**ed, **G**reen, and **B**lue colours which digital cameras capture. Viewing the rainbow – red, green, and blue go, in that order, from longest to shortest wavelength.

If not corrected for, the different wavelengths of a white point of light may not converge exactly at the sensor.

If the green component of the light (medium wavelength) converges exactly at the sensor, then the red component (longer wavelength) would converge beyond the sensor. And, the blue component (shorter wavelength) would converge in front of the sensor.

So the red and blue colours would have a slightly larger circle of confusion. They would be slightly blurred, while the green colour would stay in focus. Which would lead to a purple (red + blue) fringing - otherwise known as axial chromatic aberration.

[![](https://www.keheka.com/content/images/2023/08/Purple_Fringe_1.jpg)](https://www.keheka.com/content/images/2023/08/Purple_Fringe_1.jpg)

[![](https://www.keheka.com/content/images/2023/08/Purple_Fringe_2.jpg)](https://www.keheka.com/content/images/2023/08/Purple_Fringe_2.jpg)

Axial chromatic aberration, or purple fringing.

There are more lens effects that can trace their roots back to the circle of confusion, which we will look at later. But hopefully you’ve got a good idea now of what the circle of confusion is, and why it’s important to us.

Before we dive into each of the individual lens effects and how to replicate them in Nuke, let’s build a timeline of how light travels as it’s being photographed.

That way, we can composite the effects in the right order.

# Sequence Of Events

When you shoot a scene, the light goes on quite a journey before it gets captured as an image.

Essentially, it travels from the scene to the lens, then through all of the elements inside of the lens body, and finally to the sensor inside of the camera where the picture is formed.

However, along this route the light rays:

- Encounter several physical obstacles that block or partially block the light.

- Are bent and redirected through the curved glass elements in the lens.

- Reflect off the interior of the lens body and glass.

- Split and scatter across the sensor.

Although the light moves so fast that this journey seems instantaneous, the different lens effects occur sequentially at different stages of the light's journey.

And so:

**There is a certain order in which lens effects should be layered when we composite them in Nuke.**

### Order Of Operations

The rest of this guide is laid out in the order in which to apply the lens effects to your composite.

Essentially, we will follow the light on its way through the lens, and apply the lens effects sequentially.

The lens effects are grouped into five main categories:

**Format Effects**

- Aspect Ratio

**Environmental Effects**

- Lens Contaminations

**Additional Lighting Effects**

- Lens Flares

- Glints

- Diffusion

- Diffraction

- Filter Effects

**Natural Lens Effects**

- Lens Distortion

- Lens Expansion/Compression

- Lens Breathing

- Depth Of Field

- Bokeh

- Field Curvature

- Vignetting

**Optical Aberrations**

- Spherical Aberration

- Coma

- Halation

- Astigmatism

- Chromatic Aberration

Keep this timeline of events in the back of your mind as we look at each lens effect in detail, starting at the top with the first category:

# Format Effects

The type of lens used for capturing a shot has a large impact on the resulting image.

One property in particular affects a number of the other lens effects:

### Aspect Ratio

Two classes of lenses are typically used in production: spherical and anamorphic.

[![](https://www.keheka.com/content/images/2023/08/Spherical.png)](https://www.keheka.com/content/images/2023/08/Spherical.png)

[![](https://www.keheka.com/content/images/2023/08/Anamorphic.png)](https://www.keheka.com/content/images/2023/08/Anamorphic.png)

Spherical lens vs. anamorphic lens.

### Spherical

Spherical lenses are more common, and are the assumed lens type unless specified otherwise.

These lenses project images onto the camera sensor without affecting their aspect ratio.

If you for example were to film a square shape, it would look like a square shape in the captured image.

### Anamorphic

Anamorphic lenses, on the other hand, project a version of the image that is compressed along the width of the frame.

> [!important]
> 
> The compression is usually by a factor of two, however there are many variations of the ratio such as 1.8:1.

Footage shot on anamorphic lenses therefore requires subsequent stretching, in post-production or at the projector, in order to be displayed properly.

If you were to film the same square shape as before using an anamorphic lens, and view the captured image as-is (without desqueezing it), the shape wouldn't look square anymore but instead look like a stretched, tall rectangle.

> [!important]
> 
> The word anamorphic and its derivatives stem from the Greek anamorphoun ("to transform"), compound of morphé ("form, shape") with the prefix aná ("back, against").

You may think, why go through the hoops of capturing a squeezed image, only to desqueeze it later anyway? Why do these anamorphic lenses exist?

Anamorphic lenses were originally designed so that wide format imagery would fully utilise the film area of standard 35 mm frames.

Otherwise, wide format imagery would have left the top and bottom of the frame unused (i.e. lower vertical resolution), and would have required cropping these areas out using masks in the projector:

[![](https://www.keheka.com/content/images/2023/08/Widescreen.png)](https://www.keheka.com/content/images/2023/08/Widescreen.png)

[![](https://www.keheka.com/content/images/2023/08/Anamorphic_Widescreen.png)](https://www.keheka.com/content/images/2023/08/Anamorphic_Widescreen.png)

Widescreen (left) vs. anamorphic widescreen (right).

Anamorphic lenses improved image quality by both enhancing vertical resolution and reducing the appearance of grain.

> [!important]
> 
> As is the case with so much of technology, anamorphic lenses were also invented for military purposes. During World War I, Henri Chrétien invented this new kind of optics to provide a wide-angle viewer for military tanks. Chrétien’s lens was capable of showing a field of view  
> of 180 degrees. After the war, his anamorphic technology was first used in a cinematic context by Claude Autant-Lara in 1927 in the short film, _To Build a Fire_, based on the 1908 Jack London story by the same name.

### Practical Uses Of Aspect Ratio

With modern digital camera sensors, which have a wider aspect ratio than 35 mm film, spherical lenses often record sufficiently wide images with minimal to no cropping.

And so nowadays, the main reason to use anamorphic lenses with digital sensors is for their particular cinematic look:

- Lens flares and bokeh appear elongated as opposed to circular (unless the lens has a specially-designed ovular iris).

- Lens flares also appear as blue horizontal or vertical streaks which span  
    the entire frame. (Different lens coatings can produce different  
    coloured lens flares).

- Vignettes appear oval (although this shape can also be emulated in post-production).

- The depth of field is typically shallower – but only because you have to  
    use a longer focal length with anamorphic lenses in order to achieve the same angle of view as with a spherical lens.

[![](https://www.keheka.com/content/images/2023/08/Anamorphic_Lens_Flare.png)](https://www.keheka.com/content/images/2023/08/Anamorphic_Lens_Flare.png)

Anamorphic lens flare.

[![](https://www.keheka.com/content/images/2023/08/Spherical_Bokeh.jpg)](https://www.keheka.com/content/images/2023/08/Spherical_Bokeh.jpg)

[![](https://www.keheka.com/content/images/2023/08/Anamorphic_Bokeh.jpg)](https://www.keheka.com/content/images/2023/08/Anamorphic_Bokeh.jpg)

Spherical bokeh (left) vs. anamorphic bokeh (right).

### How To Set The Aspect Ratio In Nuke

The Viewer in Nuke automatically recognises anamorphic clips and displays them with the correct aspect ratio.

If for any reason you want to display an anamorphic clip with a 1:1 aspect ratio, right-click in the Viewer displaying the clip and enable the _Ignore Pixel Aspect_ checkbox, or use the _Ctrl/Cmd + Shift + P_ keyboard shortcut.

[![](https://www.keheka.com/content/images/2023/08/Ignore_Pixel_Aspect.png)](https://www.keheka.com/content/images/2023/08/Ignore_Pixel_Aspect.png)

Ignoring the pixel aspect ratio in Nuke.

If you want to change the pixel aspect ratio in Nuke, you can do that in the _Edit format_ menu:

[![](https://www.keheka.com/content/images/2023/08/Pixel_Aspect.jpg)](https://www.keheka.com/content/images/2023/08/Pixel_Aspect.jpg)

Changing the format's pixel aspect ratio.

> [!important]
> 
> You can access the Edit format menu by clicking on the _edit_ option at the bottom of the _output format_ drop-down menu in the Reformat node, or at the bottom of the _full size format_ drop-down menu in the Project Settings (Press S while hovering over the Node Graph).

The _pixel aspect_ knob is a width/height ratio. So if you wanted to change the standard square pixel format (1:1 = 1) to a 1.8:1 pixel aspect ratio, for example, you would change the 1 to a 1.8.

Like mentioned above, Nuke sets this automatically for you when importing footage. But I have found it useful to be able to change it manually in the past, for example when the matte painter accidentally exported the matte painting with a 2:1 pixel aspect ratio from Photoshop, and the show used a 1.8:1 pixel aspect ratio.

# Environmental Effects

A physical camera is affected by the weather and the environment.

And so different things can hit the lens and contaminate the image:

### Lens Contaminations

Light that’s travelling through the lens does not only interact with  
the lens itself, but with a whole range of things that may be stuck on  
or inside the lens, such as:

- Dirt

- Hair

- Dust

- Frost

- Snow

- Debris

- Smudges

- Scratches

- Fingerprints

- Water spots

- Rain droplets

- Condensation

- Sand build-up

- Gore/blood spurts

These environmental effects create unique marks, textures, distortions, or other artefacts in the image.

A piece of dirt will block some light from passing through the lens, for example. And a water spot might look brighter than the background when the light from the scene hits it at the right angle. Or even distort light passing through it.

And although they’re more protected than the front of the lens – the rear element of a lens, the inside of the lens, and the camera sensor can also be affected by the contaminations listed above.

[![](https://www.keheka.com/content/images/2023/08/Lens_Dirt_Front_Element.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Dirt_Front_Element.jpg)

[![](https://www.keheka.com/content/images/2023/08/Lens_Dirt_Rear_Element.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Dirt_Rear_Element.jpg)

Dirt on the front element of a lens and on the rear element of a lens.

[![](https://www.keheka.com/content/images/2023/08/Sensor_Dirt.jpg)](https://www.keheka.com/content/images/2023/08/Sensor_Dirt.jpg)

Dirt on the camera sensor.

Because the scene that the camera is focusing on is a certain distance away from the lens, nothing that is stuck on the front of the lens will be in focus.

> [!important]
> 
> The circle of confusion for objects on or very near the front of the camera lens is much larger than a point – More info in the _Depth Of Field_ section.

A piece of dirt or a water droplet on the lens will just show up as a blurry blob in the image, for example.

> [!important]
> 
> Any bokeh may still appear sharp, though.

[![](https://www.keheka.com/content/images/2023/08/Lens_Water_Spot.png)](https://www.keheka.com/content/images/2023/08/Lens_Water_Spot.png)

A photograph affected by a water drop on the lens.

The closer the dust and the dirt is to the sensor, i.e. to the rear focal plane, the sharper and more defined it will be in the image:

[![](https://www.keheka.com/content/images/2023/08/Lens_Dirt.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Dirt.jpg)

A photograph affected by dust on the lens and on the sensor.

Keep in mind that many of the environmental effects will mostly be noticeable in more extreme conditions. For example, against a bright background like the sky, or when a light is shining at the camera and a lens flare blooms.

Otherwise, a very soft and semi-transparent piece of dust may barely be visible or noticeable in the image.

> [!important]
> 
> Lens contaminations are often subtle effects that only briefly become visible as light hits them at the right angle.

Nevertheless, adding environmental effects to your composite can help give it that extra few percent of realism that will take it across the finish line.

### Practical Uses Of Lens Contaminations

You can use environmental effects to:

- Dirty up 'perfect' CG renders so it looks like they were filmed with a real camera.

- Make ‘archival’ footage look more weathered or aged, and give it more authenticity.

- Make footage look more grungy or vintage as part of the film’s aesthetic.

You can also use environmental effects to get the viewer right into the action. For example:

- First person view on a ship out in stormy seas → add water spray on the lens.

- Closeup of a brutal fight → add blood splatter on the lens.

- ‘Found footage’ of a natural disaster like a tornado → add debris which hits the lens and cracks it.

- POV of dirt bikes racing on a track → add mud splashes on the helmet visor.

### How To Create Lens Contaminations In Nuke

The most common way of adding lens contaminations to your composite is by using footage and textures from your element library.

As an example, ActionVFX has:

- An [Optics category](https://www.actionvfx.com/collections/optics/category?ref=keheka.com) with lens dirt.

- A [Weather category](https://www.actionvfx.com/collections/weather/category?ref=keheka.com) with rain and water running down the lens.

- A [Blood & Gore category](https://www.actionvfx.com/collections/blood-gore/category?ref=keheka.com) with blood splatter.

There are plenty of other element providers as well, such as:

- [Pixel Lab](https://www.thepixellab.net/vfx-elements-bundle-pl?ref=keheka.com)

- [Footage Crate](https://footagecrate.com/?ref=keheka.com)

- [FX Elements](https://www.fxelements.com/?ref=keheka.com)

Or, you can shoot your own elements. Some [tips at the end of this article](https://nofilmschool.com/2017/04/exclusive-10-lifesaving-vfx-tips-for-your-indie-film?ref=keheka.com), and a [behind-the-scenes video](https://youtu.be/6GBlVDkZb9g?ref=keheka.com) of some guys shooting blood elements.

There are also various tools on Nukepedia for creating lens contaminations in Nuke, for example [RainMaker](https://www.nukepedia.com/gizmos/particles/rainmaker?ref=keheka.com).

> [!important]
> 
> Another way is to make particle setups in Nuke with snow, rain, sand, spray, etc. hitting the lens. But that's a whole other tutorial in itself.

Or, you can take a more procedural 2D approach and use for example Noise nodes to create lens contaminations:

[![](https://www.keheka.com/content/images/2023/08/Lens_Blood_Noise.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Blood_Noise.jpg)

Blood splatter on the lens created using three Noise nodes and some grading.

Just create a few Noise nodes and merge (mask) them together. Adjust the _x/ysize_, _z_, and _gain/gamma_ values in the Noise nodes until you’ve created a blob shape with some detail in it.

Then, grade it red and merge it over your composite. Pretty simple!

[![](https://www.keheka.com/content/images/2023/08/Lens_Blood_Noise_Setup.png)](https://www.keheka.com/content/images/2023/08/Lens_Blood_Noise_Setup.png)

Setup for creating blood splatter on the lens.

> [!important]
> 
> Find an interesting texture and multiply it with – or mask it by – the setup above for even more detail.

# Additional Lighting Effects

When shooting a scene, additional lighting effects that may or may not be intentional can affect your image.

You may get non-image-forming light contributing to (or polluting) your image in addition to the actual image-forming light which depicts the scene.

  

> [!important]
> 
> Non-image-forming light is light that reaches the camera sensor without forming a particular image. It can leave visible artefacts, glows, and halos, or just wash out regions of the image, or the image overall.

Let’s take a look, starting with lens flares:

### Lens Flares

A lens flare is caused by a light source (e.g. the sun) that is much brighter than the rest of the scene and what the camera is set to record.

When the bright light is either within the frame or just outside of it – still hitting the front element of the lens – the light rays reflect and bounce around inside of the lens:

[![](https://www.keheka.com/content/images/2023/08/Lens_Flare_Diagram.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Flare_Diagram.jpg)

Lens flare diagram. Only two lens elements are shown for simplicity but imagine the reflections happening with for example a dozen or more elements, which is common in modern lenses.

Lens flares can be divided into two distinct parts which usually go hand in hand: **veiling glare** and **ghosts**.

Both are caused by non-image-forming light reaching the camera sensor, but each one travels a different path through the lens and takes on a unique appearance.

  

> [!important]
> 
> Lens manufacturers usually apply several extremely thin layers of Magnesium Fluoride or other special coatings to the surface of the lens in order to reduce lens flares. Each layer helps eliminate the reflections of a specific wavelength of light. But even with these multi-coated lens elements, lens flares can still occur.

### Veiling Glare

Veiling glare makes the image look hazy and washed out by reducing contrast and saturation.

It adds light to dark regions, and white to saturated regions:

[![](https://www.keheka.com/content/images/2023/08/Veiling_Glare.jpg)](https://www.keheka.com/content/images/2023/08/Veiling_Glare.jpg)

[![](https://www.keheka.com/content/images/2023/08/No_Veiling_Glare.jpg)](https://www.keheka.com/content/images/2023/08/No_Veiling_Glare.jpg)

A veiling glare is washing out the image in the first example. The sun is shielded in the second example, removing the veiling glare and restoring contrast and saturation.

Veiling glare occurs when light scatters within the camera lens.

Incoming light rays stray from the normal image-forming paths due to reflections from the lens elements, diaphragm blades, and lens body interior, as well as dirt, film, or scratches on the lens surfaces.

The scattered light disperses across the sensor where it gets captured as additional light in the image – which may or may not be wanted.

### Ghosts

Ghosts are visible artefacts, often in the shape of the aperture, which are caused by internal reflections within the lens.

[![](https://www.keheka.com/content/images/2023/08/Lens_Flare_Ghosts.jpg)](https://www.keheka.com/content/images/2023/08/Lens_Flare_Ghosts.jpg)

Multiple ghosting artefacts in a row.

Ghosts are formed when light follows a pathway through the lens that contains one or more reflections from the lens surfaces.

They can appear in the image as several:

- Streaks

- Circles

- Polygonal shapes

- Starbursts

- Rings

> [!important]
> 
> Zoom lenses usually have more lens elements inside of them than prime lenses, which means there are more opportunities for reflections in a zoom lens. Which again means that more ghosts can show up in the image when using a zoom lens.

The ghost patterns typically spread out widely across the frame and change location with the camera's movement relative to the light sources. They track with the light’s position and fade as the camera points away from the bright light.

As mentioned, the shape of the aperture plays a  
part when it comes to the look of the ghosts. If the lens has a 6-bladed aperture, for example, the ghosts may have a hexagonal pattern.

### Practical Uses Of Lens Flares

Lens flares are often deliberately used to invoke a sense of drama in the shot.

Take for instance when the subject of the shot is the light source itself – for example an explosion, a burning comet, or a bright UFO landing. In those cases, lens flares are usually desirable and can be used for dramatic effect, emphasising the lighting and atmosphere in the shot.

Lens flares can indicate that something spectacular is happening just off camera, and add an interesting organic layer to the image.

They’re also useful for adding a sense of realism to your composite, implying  
that the shot is an unedited original of the real life scene.

### How To Create Lens Flares In Nuke

There are many great lens flare tools for Nuke.

Some are highly customisable and require a bit of manual work – like tracking the hotspot in the image. Others handle things more automatically.

Here are some solid options:

- [Optical Flares](https://www.videocopilot.net/products/opticalflaresnuke/?ref=keheka.com)

- [FlareFactory Plus](https://www.nukepedia.com/gizmos/draw/flarefactory-plus?ref=keheka.com)

- [H_AutoFlare](https://www.nukepedia.com/gizmos/image/h_autoflare?ref=keheka.com)

- [AutoFlare](https://www.nukepedia.com/gizmos/filter/autoflare?ref=keheka.com)

Another way to add lens flares is to use pre-recorded ones from your elements library, and merge (plus)’ing them with your composite. If you do, make sure that the element you’re using physically makes sense in relation to your scene, i.e. that the hotspot matches and the ghosts line up in the right orientation.

If your hotspot is at the top right like in the ghosting artefacts example image above, then the ghosts will spread diagonally across the frame down to the left. If it’s at the top left, then they would spread diagonally down to the right. If it’s at the bottom left they would spread diagonally to the top right, and so on.

> [!important]
> 
> As covered in the _Aspect Ratio_ section, keep the format of your image in mind with regards to which lens flares you use. If you have a shot filmed on a spherical lens with circular bokeh, it doesn’t make sense to add an anamorphic (squeezed) lens flare with the recognisable blue streaks across the frame. And vice versa.

Lens flares, especially the veiling glare component, are often quite organic-looking and nondescript.

In some shots, particularly where the camera isn't moving much, you can often get away with just adding a still frame of a lens flare from your elements library to your composite and animating the mix up and down as needed.

[![](https://www.keheka.com/content/images/2023/08/Organic_Lens_Flare_1.jpg)](https://www.keheka.com/content/images/2023/08/Organic_Lens_Flare_1.jpg)

[![](https://www.keheka.com/content/images/2023/08/Organic_Lens_Flare_2.jpg)](https://www.keheka.com/content/images/2023/08/Organic_Lens_Flare_2.jpg)

[![](https://www.keheka.com/content/images/2023/08/Organic_Lens_Flare_3.jpg)](https://www.keheka.com/content/images/2023/08/Organic_Lens_Flare_3.jpg)

Organic, fairly nondescript lens flares.

However, to match a specific lens flare in your shot, break the flare down into each of its individual elements and recreate them separately using Flare nodes:

[https://vimeo.com/62583847](https://vimeo.com/62583847)

Manually creating a custom lens flare.

### Glints

Glints are small, localised lens flares caused by bright lights and their reflections in shiny objects.

They appear as small starbursts or mini-glares at the hotspots in the image.

[![](https://www.keheka.com/content/images/2023/08/Glint_Chandelier.jpg)](https://www.keheka.com/content/images/2023/08/Glint_Chandelier.jpg)

Glints on a chandelier.

[![](https://www.keheka.com/content/images/2023/08/Glint_Solar_Panel.png)](https://www.keheka.com/content/images/2023/08/Glint_Solar_Panel.png)

The sun’s reflection on a solar panel.

Glints can also be softer, more glare-like. On the screen left side edge of the diamond below, a bright hotspot causes a mini-glare in the image:

[![](https://www.keheka.com/content/images/2023/08/Glint.png)](https://www.keheka.com/content/images/2023/08/Glint.png)

Glint on a diamond.

Glints are typically not strong enough to cause ghosting artefacts like a full-on lens flare does. Instead, they appear as fairly small glows, streaks, or starbursts on the actual hotspot in the image.

When the camera, objects, and/or light sources move around in the scene, glints may flare up and down in brightness, making the objects shimmer.

### Practical Uses Of Glints

Glints can strategically be used to grab the attention of the audience, guiding them where to look.

For example, in a scene in Mission Impossible: Dead Reckoning Part One, Ethan is in the Yemen desert and tries to communicate with Ilsa, who is far off in a deserted compound, by using a compass reflecting the sun.

From her point of view we would normally not see Ethan, who is far away behind the desert dunes. And so adding a bright glint (from the compass) shows the audience where he is in relation to Ilsa.

Glints can be overt like this, but they should be motivated by something tangible. You’ve probably seen toothpaste commercials where the actor smiles and their teeth sparkle. It’s obviously done to sell the idea that using that particular toothpaste will make your teeth white. Similarly, you might have seen ads where jewellery sparkles in the light, to enhance their beauty.

However, glints that are too strong and out of place can quickly look cheesy. And so, err on the side of subtlety.

### How To Create Glints In Nuke

Creating glints in Nuke is quite simple because there is a Glint node specifically designed for this task.

The Glint node can simulate the little starbursts you see in glints (increase the _no. of rays_ value):

[![](https://www.keheka.com/content/images/2023/08/Glints_Glint_Node.png)](https://www.keheka.com/content/images/2023/08/Glints_Glint_Node.png)

Three variations of glints (zoomed in) created using the Glint node in Nuke.

In the example above, I painted a white dot using a RotoPaint node, and connected the Glint node to it. Then, I played around with the Glint settings to get the three different looks, and added a touch of exponential glow and chromatic shift.

You could merge (plus) something similar to this with your composite, and even animate  
the Glint settings to make the glints shimmer.

If you have several points of light in your image, you can connect the Glint node directly to your image and it will create multiple starbursts. However, it can pretty quickly go a bit haywire.

A better way is to:

1. Luma key your image using a Keyer node, creating an alpha of only the brightest luminance values (i.e. raising the A slider).

1. Premultiply the image using this alpha.

1. Add the Glint node to the premultiplied result.

1. Merge (plus) the Glint result with your composite.

That way, you can control the areas where the glints show up.

> [!important]
> 
> The Glint node doesn’t have an ‘effect only’ toggle. If merging the Glint result with your composite makes it too bright, subtract the original values first: Merge (from) the premultiplied image (A) from the Glint result (B) before merging (plus) that with your composite.

With the luma key setup, you also have the option of masking the luma keyed alpha by an animated Noise pattern and creating a shimmering effect. Or, masking the effect by a roto to only affect a specific area, like for example that one glint on the diamond above.

### Diffusion

Diffusion is an effect which softens and scatters light.

Diffused light, or soft light, is light that's been filtered by something. Sunlight through a sheer curtain is diffused, for example. And light from behind a lampshade is diffused compared to the direct light of a bare bulb.

With cameras, you can attach diffusion filters  
to the lens which, as their name suggests, diffuses the light coming in  
from the scene.

A diffusion filter lifts the contrast and softens the image, and adds a soft glow around the highlights in the image:

[![](https://www.keheka.com/content/images/2023/08/Diffusion_Gentle.jpg)](https://www.keheka.com/content/images/2023/08/Diffusion_Gentle.jpg)

No filter applied on the left vs. a gentle diffusion filter applied on the right.

[![](https://www.keheka.com/content/images/2023/08/Diffusion_Harry_Potter.jpg)](https://www.keheka.com/content/images/2023/08/Diffusion_Harry_Potter.jpg)

Heavily diffused light in Harry Potter and the Deathly Hallows, Part 2.

### Pro Mist

‘Pro Mist’ is often used interchangeably with diffusion as a term in visual effects.

Pro Mist is a type of diffusion filter commonly used in photography and cinematography. It’s designed to soften the image, reduce contrast, and create a subtle glow or halation effect.

These filters are made with a combination of clear and diffusing elements, which scatter and slightly blur the light entering the lens. They come in various different strengths; some adding more of the above characteristics than others.

[![](https://www.keheka.com/content/images/2023/08/Promist.png)](https://www.keheka.com/content/images/2023/08/Promist.png)

Comparison between no filter and a (very strong) Pro Mist filter attached to the lens.

### Practical Uses Of Diffusion

Diffusion can be used to create a soft, dreamy, or romantic look in your shot by reducing contrast and blurring details.

Diffusion filters also reduce blemishes and wrinkles on the actors.

They can add a delicate and ethereal quality to the image, as well. Look for instance at the scene where Frodo wakes up in Rivendell in The Lord Of The Rings: The Return Of The King:

[![](https://www.keheka.com/content/images/2023/08/Diffusion_Frodo.png)](https://www.keheka.com/content/images/2023/08/Diffusion_Frodo.png)

The light is heavily diffused to create that otherworldly soft, glowing look.

### How To Create Diffusion In Nuke

Creating diffusion in Nuke can be as simple as adding a Glow to your  
image with a high tolerance/threshold and a small to medium size.

That way, only the brightest values in the image get that extra little glow.

> [!important]
> 
> Play with the x/y size of the glow to create a more irregular look.

For more control, you can use the same approach described previously for creating glints, but with a glow instead:

1. Luma key your footage.

1. Premultiply it by the luma keyed alpha.

1. Add the glow (set to output the effect only).

1. Merge (plus) the result back over your composite.

That way, you can mask the luma keyed alpha by an animated Noise pattern and create a shimmering effect, for example. You can also mask it by a roto in order to only affect a specific area.

> [!important]
> 
> Make sure to use an exponential glow node from Nukepedia, for a more natural light falloff:

Exponential glow gizmos:

- [ApGlow](https://www.nukepedia.com/gizmos/filter/apglow?ref=keheka.com)

- [bm_OpticalGlow](https://www.nukepedia.com/gizmos/filter/bm_opticalglow?ref=keheka.com)

- [L_ExponBlur](https://www.nukepedia.com/gizmos/filter/l_exponblur?ref=keheka.com)

> [!important]
> 
> Another, simple way of adding a gentle diffusion, for example to CG renders to  
> take the 'perfect' edge off, is to connect a Blur node and set it to a _size_ of ~3 and a _mix_ of ~0.5.

### Diffraction

Diffraction is a physical phenomenon where the smaller an opening gets, the more the light waves passing through it, or reflecting off of it, collide and interfere with one another.

This can lead to interesting effects that you can see in everyday life. For example, the closely spaced tracks on a CD or DVD act as a diffraction grating to form the familiar rainbow pattern seen when looking at a disc.

> [!important]
> 
> Sound waves can also diffract around objects, which is why you can still hear someone calling even when hiding behind a tree.

Your TV or phone screen may also show a rainbow pattern in bright reflections, due to the fine pixel patterns on the screens:

[![](https://www.keheka.com/content/images/2023/08/Diffraction.jpg)](https://www.keheka.com/content/images/2023/08/Diffraction.jpg)

Light diffracting off the fine pixel pattern on a phone screen.

As the waves spread out and are filmed, the light rays end up spilling over multiple pixels on the sensor, causing colourful patterns.

### Practical Uses Of Diffraction

Diffraction can be used to add more realistic reflections to all sorts of monitors or other objects with fine grid patterns.

It can also be used as an interesting motion graphics effect, or to create a kaleidoscope:

[![](https://www.keheka.com/content/images/2023/08/Diffraction_1.png)](https://www.keheka.com/content/images/2023/08/Diffraction_1.png)

[![](https://www.keheka.com/content/images/2023/08/Diffraction_2.jpg)](https://www.keheka.com/content/images/2023/08/Diffraction_2.jpg)

Kaleidoscopic effects created using diffraction grating lenses.

To create diffraction in Nuke, it’s useful to first become familiar with how the Convolve node works and what an Airy disk is:

### Convolve

The Convolve node lets you create custom filter effects by supplying your own filter image.

The filter image is used as the convolution matrix. That means, the new value of a pixel is calculated by centering the filter image on the pixel, examining its neighbours, multiplying each pixel value by the corresponding pixel values in the filter image, and then adding the results together.

This allows you to defocus a shot and create lens blur effects in the shape of the filter image, for example:

[![](https://www.keheka.com/content/images/2023/08/Convolved_Image_Heart.png)](https://www.keheka.com/content/images/2023/08/Convolved_Image_Heart.png)

Image convolved with a heart shaped filter.

The filter I used in the image above was just a tiny heart I drew with a RotoPaint node:

[![](https://www.keheka.com/content/images/2023/08/Convolution_Filter_Heart.png)](https://www.keheka.com/content/images/2023/08/Convolution_Filter_Heart.png)

Heart shaped convolution filter.

By getting creative with the filters you supply the Convolve node, you can emulate a lot of interesting lens effects.

– Including diffraction, which requires a custom Airy disk:

### Airy disk

An [Airy disk](https://en.wikipedia.org/wiki/Airy_disk?ref=keheka.com) describes the best-focused spot of light, i.e. the smallest circle of confusion, that a perfect lens with a circular aperture can make, limited by the diffraction of light.

When light is diffracted, the Airy disk starts to look like a spot surrounded by coloured concentric circles, or an Airy pattern.

### How To Create Diffraction In Nuke

By using Expression nodes in Nuke, we can create concentric circles for each colour channel, add them together with a Radial node for the centre spot, and form an Airy pattern:

[![](https://www.keheka.com/content/images/2023/08/Convolution_Filter_Diffraction_.png)](https://www.keheka.com/content/images/2023/08/Convolution_Filter_Diffraction_.png)

Diffraction convolution filter.

And then, just feed this filter into a Convolve node to create diffraction.

It’s a little bit of a larger setup to build the filter, so I will instead link to a great gizmo that you can jump into (select the group and press Ctrl + Enter) and take a look:

[VirtualLens](https://www.nukepedia.com/gizmos/filter/virtuallens?ref=keheka.com)

### Filter Effects

There is a large variety of filters that can be attached to a lens to apply creative effects to the image.

Some examples include:

- [https://www.vid-atlantic.com/products/flare-streak-filters](https://www.vid-atlantic.com/products/flare-streak-filters?ref=keheka.com)

- [https://prismlensfx.com/collections/filters](https://prismlensfx.com/collections/filters?ref=keheka.com)

- [https://rigu.co.uk/prism-photography](https://rigu.co.uk/prism-photography?ref=keheka.com)

### Practical Uses Of Filter Effects

These filters can be used to create a certain unique look in the  
image, such as star patterns around bright objects, or kaleidoscopic  
imagery.

Or, they can be used to emulate lens effects  
found in higher-end setups, such as the streaky lens flares seen when  
using anamorphic cinema lenses.

### How To Create Filter Effects In Nuke

A filter effect is typically some variant of the lens effects already covered in the Additional Lighting Effects sections above, or in the Optical Aberrations sections further below.

Please see those sections for tips on how to create a particular filter effect. (There are hundreds of different filter effects, and so it’s unfortunately not feasible to cover how to create them all in this  
guide).

# Natural Lens Effects

These effects are natural changes to the image caused by the lensing body itself:

### Lens Distortion

Lens distortion is a type of distortion that causes straight lines to appear curved or bent in the image.

It usually occurs towards the edges of the frame. That's because the lens elements are more curved towards the outer edges, and bend the light more there than they do in the centre.

We usually divide lens distortion into two main types:

- Barrel lens distortion

- Pincushion lens distortion

…And two additional special cases:

- Moustache lens distortion

- Fisheye lens distortion

They each display unique properties:

### Barrel Distortion

Barrel distortion is most common in wide angle lenses and it makes straight lines bow outward from the centre of the image.

[![](https://www.keheka.com/content/images/2023/08/Barrel_Distortion.jpg)](https://www.keheka.com/content/images/2023/08/Barrel_Distortion.jpg)

Barrel distortion curving the straight lines in the image outward.

> [!important]
> 
> Barrel distortion got its name because it makes the straight lines in the image bend in the shape of a barrel.

### Pincushion Distortion

Pincushion distortion is most common in telephoto lenses and it makes straight lines bow inward toward the centre of the image.

[![](https://www.keheka.com/content/images/2023/08/Pincushion_Distortion.jpg)](https://www.keheka.com/content/images/2023/08/Pincushion_Distortion.jpg)

Pincushion distortion curving the straight lines in the image inward.

> [!important]
> 
> Pincushion distortion got its name because it makes the straight lines in the image bend in the shape of a cushion.

### Moustache Distortion

Moustache distortion is most common in super telephoto lenses, and is a combination of a barrel distortion and a pincushion distortion.

It makes straight lines bow outward near the centre of the image (barrel  
distortion), and inward near the edges of the frame (pincushion  
distortion).

[![](https://www.keheka.com/content/images/2023/08/Moustache_Distortion.jpg)](https://www.keheka.com/content/images/2023/08/Moustache_Distortion.jpg)

Moustache distortion: a barrel distortion near the centre of frame combined with a pincushion distortion at the edges of the frame.

> [!important]
> 
> Moustache distortion got its name because it makes the horizontal lines in the image bend in the shape of a moustache.

### Fisheye Distortion

Fisheye distortion happens in special fisheye lenses (ultrawide lenses) and is an extreme form of barrel distortion.

It makes straight lines bow heavily outward from the centre of the image, leaving almost no straight lines at all.

[![](https://www.keheka.com/content/images/2023/08/Fisheye.png)](https://www.keheka.com/content/images/2023/08/Fisheye.png)

Fisheye lens distortion.

> [!important]
> 
> Fisheye distortion got its name from the large, ultrawide lenses that cause it, which look like the eyes of a fish.

### Practical Uses Of Lens Distortion

Lens distortion should be added to your CG renders and matte painting or other projections, in order for them to line up correctly with the scan.

Without lens distortion they may appear to slide or be misaligned near the edges of the frame.

Lens distortion can also be exaggerated and used as a creative effect. For example, to [indicate that someone is on drugs](https://youtu.be/JKpRvCi9KmY?ref=keheka.com) and their vision goes strange in a POV shot.

Or, to create an interesting [motion graphics effect](https://youtu.be/02LFp416IqY?ref=keheka.com).

### How To Create Lens Distortion In Nuke

The matchmove department will normally provide a lens distortion node, ST map, or other data for the lens distortion of a shot.

If you need to calculate lens distortion yourself in Nuke, you can do that using the LensDistortion node: [Tutorial](https://learn.foundry.com/nuke/content/comp_environment/lens_distortion/adding_removing_lens_distortion.html?ref=keheka.com).

You can also camera track your shot in Nuke using the CameraTracker node and export a pre-calculated LensDistortion node: [Tutorial](https://learn.foundry.com/nuke/content/comp_environment/cameratracker/camera_tracking.html?ref=keheka.com).

**No matter how you get your lens distortion, it's important to know the correct workflow for applying it:**

In the vast majority of cases, we want to keep our scan as pristine as possible. That means we don't want to apply all sorts of unnecessary filtering to it which softens the pixels and causes a loss of detail.

Which means, we do **NOT** typically Undistort our scan, merge our CG with it, and then Redistort everything.

Instead, the correct procedure is to leave the scan as-is, ONLY apply the lens distortion to the CG, and merge that over the scan.

That way, only the CG takes a filter hit. Which is inevitable and absolutely fine.

> [!important]
> 
> The exception to this is when you need to reproject your scan, for example to change the camera move, or use a part of the scan somewhere else. In that case, you would undistort the scan, project it, and redistort the projection.

### Lens Expansion/Compression

When you change the focal length, you visually expand or compress space in the image.

A short focal length (wide lens) opens up the space and exaggerates the sense of depth, while a long focal length (telephoto lens) compresses space and flattens the perspective.

> [!important]
> 
> By compression I mean objects visually appear closer together and the background seems flatter when using a telephoto lens.

[![](https://www.keheka.com/content/images/2023/08/Wide_Angle_Vs_Telephoto.jpg)](https://www.keheka.com/content/images/2023/08/Wide_Angle_Vs_Telephoto.jpg)

The same person shot using a wide angle lens versus a telephoto lens.

### Practical Uses Of Lens Expansion/Compression

Looking at the image comparison example above, it’s clear that  
different focal lengths create a different look and feel to the image.

And so lens expansion/compression is used as a tool to tell the story in a certain way, and to evoke certain emotions in the audience.

This article explores [How Focal Length Alters the Psychological Impact of Your Images](https://www.premiumbeat.com/blog/various-focal-lengths-for-images/?ref=keheka.com).

Wide angle lenses can for example make you feel like you are in the middle of the film’s action. Used in close proximity to a subject, the result is a more immersive film.

Telephoto lenses, however, create a somewhat objective perspective that makes the audience feel more like an observer.

As compositors, it’s part of our job to make sure our work is consistent with the intention behind the shot.

### How To Avoid Lens Expansion/Compression Problems In Nuke

When working with images in Nuke, pay attention to which focal  
lengths the scans and elements that you’re using have been filmed with.

If you use an element shot on a telephoto lens – which heavily compresses space and distances – it may not look natural composited into a shot filmed with a wide angle lens, and vice versa.

In the image comparison example above, you can see how space is flattened and compressed when using a telephoto lens versus how space is opened up on a wide angle lens. Compositing these next to each other in the same scene wouldn’t look right due to the dramatic difference in perspective.

### Lens Breathing

Lens breathing or focus breathing refers to a change in the angle of  
view and apparent focal length when adjusting the focus, causing the  
image to appear slightly zoomed in or out.

Here is a great video explaining the effect:

> [!info] What is lens breathing? (And why should you care?)  
> What is lens breathing?  
> [https://youtu.be/43asl1yYw2E](https://youtu.be/43asl1yYw2E)  

What is lens breathing?

And here are some real life examples of lens breathing:

> [!info] SAMPLE - LENS BREATHING  
> Breathing refers to when a lens' optics change the apparent focal length slightly when shifting the mechanical focus.  
> [https://youtu.be/bIrJUFtYxiQ](https://youtu.be/bIrJUFtYxiQ)  

Lens breathing samples.

### Practical Uses Of Lens Breathing

Lens breathing is something to be aware of when you're matching a rack focus in the scan.

Your patch may appear to slide slightly, and may need to be counter-animated with a Transform node to stay aligned.

### How To Create/Match Lens Breathing In Nuke

Like mentioned above, to create or match lens breathing you'll have to artificially zoom in to, or out from, your image.

You can achieve that by applying a Transform node to your patch with the centre point at the centre of your image, and adjusting the _scale_ value by a very small amount.

A doubling of the focal length equals a halving of the image scale, and vice versa. It's an inverse proportion.

So if the focal length increases a tiny bit during the rack focus, you may have to scale your patch down by a tiny amount to compensate.

### Depth Of Field

Depth of field is the range of distance in the scene that appears acceptably sharp in an image.

Like we covered in the Circle Of Confusion section, an image is only 100% sharp at the front focal plane of the camera: a thin slice across the camera's frustum.

In front of and beyond that plane, the focus falls off.

[![](https://www.keheka.com/content/images/2023/08/Focal_Plane.jpg)](https://www.keheka.com/content/images/2023/08/Focal_Plane.jpg)

Depth of field diagram.

The depth of field is influenced by several different properties:

- **Aperture size**: The smaller the aperture, the greater the depth of field. And vice  
    versa: the wider the aperture, the shallower the depth of field.

- **Focal length**: Wide lenses, which have a shorter focal length, tend to have a greater  
    depth of field. Long lenses, which have a longer focal length, compress  
    and isolate the subject with a shallower depth of field.

- **Distance to the subject**: The closer the subject is to the lens, the shallower the depth of field becomes. And vice versa, the further away the subject is from the lens, the wider the depth of field will be.

- **Acceptable circle of confusion size**: If you want to print out a picture as a large poster, for example, the  
    acceptable circle of confusion size is smaller than if you would be  
    printing it for a small picture frame. I.e., more of the image will have to stay sharp in the poster to get the same apparent depth of field as  
    in the small picture.

- **Sensor size** (indirectly!): Although it’s actually [the other way around](https://fstoppers.com/education/no-larger-sensors-do-not-produce-shallower-depth-field-254158?ref=keheka.com), in practice larger sensors tend to produce a shallower depth of field  
    compared to smaller ones. – Because you would have to either move closer to the subject or use a longer lens to get the same framing as you  
    would with a smaller sensor.

[![](https://www.keheka.com/content/images/2023/08/DoF_Aperture.jpg)](https://www.keheka.com/content/images/2023/08/DoF_Aperture.jpg)

A wider aperture (top) allows for a larger circle of confusion, i.e. more defocus, than a smaller aperture (bottom). And so a wider aperture will have a shallower depth of field.

> [!important]
> 
> You can play around with, and see the effects of, each camera setting in this [Depth of field simulator](https://dofsimulator.net/en/?ref=keheka.com).

And so, a picture taken…

- Using a wide aperture.

- Using a long lens.

- Up close to the subject.

- With the intention of viewing it up close/at a large size.

…will have a _much_ shallower depth of field than the same picture taken…

- Using a small aperture.

- Using a wide lens.

- Far away from the subject.

- With the intention of viewing it at a distance/at a small size.

[![](https://www.keheka.com/content/images/2023/08/DoFs.jpeg)](https://www.keheka.com/content/images/2023/08/DoFs.jpeg)

Shallow, medium, and deep depth of field.

> [!important]
> 
> There is also something called depth of _focus_, not to be confused with depth of field. It's not something we normally need to worry about, but I'll include [a link for those interested](https://www.masterclass.com/articles/depth-of-focus-vs-depth-of-field?ref=keheka.com) in learning about the difference.

### Practical Uses Of Depth Of Field

Depth of field is used as a creative tool in cinematography.

**Shallow depth of field** is best applied for:

- Creating intimacy between the subject and the audience.

- Hyper-focusing on a subject's facial expression or an object's texture.

- Separating or isolating the foreground from the background by blurring distractions.

**Medium depth of field** is best applied for:

- Telling a story between the subjects and their environment, while keeping a respectable distance.

- Focusing on 1-5 subjects or objects.

- Keeping the subjects in the foreground sharp while blurring the background.

**Deep depth of field** is best applied for:

- Conveying a sense of grandeur, such as large landscapes, crowds, or groups of objects.

- Immersing the audience in the environment.

- Focusing on subjects in both the foreground and the background at the same time.

### How To Create Depth Of Field In Nuke

I have previously covered how to create depth of field in the _Defocus In Nuke_ section of [How To Create A Sense Of Depth In Your Composite](https://www.keheka.com/how-to-create-a-sense-of-depth-in-your-composite/).

Please take a look there for the details.

However, because it’s so good, I’ll add one of the tutorials (by Jed Smith) I linked to then, again here:

> [!info] Simulating Physically Accurate Depth of Field in Nuke  
> A discussion of lenses and optics, and how they affect depth of field behavior in an image.  
> [https://youtu.be/Rv7L6M8f2lk](https://youtu.be/Rv7L6M8f2lk)  

Simulating physically accurate depth of field in Nuke.

And here is a newer tutorial from the Foundry using the Bokeh node:

> [!info] How to Get Photorealistic Depth of Field with Nuke's Bokeh Node with @AlfieVaughan  
> In this tutorial, VFX Artist Alfie Vaughan guides you through different methods for adding depth of field to a shot by defocusing the background, midground and foreground at various degrees with several of Nuke's defocus nodes.  
> [https://youtu.be/E1iXIJJnSwY](https://youtu.be/E1iXIJJnSwY)  

How to Get Photorealistic Depth of Field with Nuke's Bokeh Node.

### Bokeh

Bokeh refers to the aesthetic quality of the out-of-focus areas in an image.

It's the visual rendering of circles of confusion. Depending on how the lens elements are designed and how the aperture of the lens is shaped, the bokeh will have distinct characteristics. It can vary in shape, size, and smoothness, creating pleasing or distracting backgrounds.

Bokeh is often most visible around small background highlights, such as specular reflections and light sources:

[![](https://www.keheka.com/content/images/2023/08/Bokeh_Octagonal_Shaped.jpg)](https://www.keheka.com/content/images/2023/08/Bokeh_Octagonal_Shaped.jpg)

An octagonal aperture causing bokeh with eight sides.

[![](https://www.keheka.com/content/images/2023/08/Bokeh_Circular.jpg)](https://www.keheka.com/content/images/2023/08/Bokeh_Circular.jpg)

Circular bokeh in the centre, elliptical bokeh at the edges of the frame.

In the last image above, notice how the bokeh is changing shape toward the edges of the frame. Light rays entering the edge of the lens must be bent toward the image sensor, forming bokeh that is more elliptical in shape.

These ellipses are known as “cat’s-eye bokeh,” because the shape resembles the feline pupil.

### Practical Uses Of Bokeh

The style of bokeh in a shot can be an artistic choice as some bokeh is subjectively more pleasing aesthetically than others.

In Scott Pilgrim Vs The World, the bokeh was manipulated to subtly tell a story:

[![](https://www.keheka.com/content/images/2023/08/Scott_Pilgrim.jpg)](https://www.keheka.com/content/images/2023/08/Scott_Pilgrim.jpg)

Heart shaped bokeh subtly indicated the characters’ internal emotional states.

### How To Create Bokeh In Nuke

The Bokeh and ZDefocus nodes in Nuke are great for adding depth of field and bokeh to your image.

You can adjust the bokeh settings in the _Kernel_ tab of the Bokeh node, or down in the _filter type_ menu in the ZDefocus node.

By default, they’re set to _circular_/_disc_, however you can change this to _aperture blades_/_bladed_, or _input_/_image_, respectively.

As the name suggests, the aspect ratio option changes the aspect ratio of the bokeh, and you can adjust this to create anamorphic bokeh, for example. The aspect ratio values are inverted between the two nodes, so a value of 2 in the Bokeh node equals a value of 0.5 (1/2) in the ZDefocus node.

You can adjust the look of the bokeh quite a lot using the various settings – changing the amount of diaphragm blades, the curvature, softness, rotation, feather, etc.

However, if you want to match a specific bokeh in a scan which may have certain unique aberrations, you can feed the Bokeh/ZDefocus nodes a custom kernel/filter as an input:

[![](https://www.keheka.com/content/images/2023/08/Bokeh_0_Setup.png)](https://www.keheka.com/content/images/2023/08/Bokeh_0_Setup.png)

Setup for using a custom kernel with a Bokeh node.

First, find a single, representative bokeh kernel in your scan. One that’s preferably not too blown out, and that shows the entire shape and any aberrations you would like to keep in the bokeh:

[![](https://www.keheka.com/content/images/2023/08/Bokeh_1_Selection.png)](https://www.keheka.com/content/images/2023/08/Bokeh_1_Selection.png)

Deciding on a bokeh kernel to use.

Then, isolate it using a luma key and a roto mask:

[![](https://www.keheka.com/content/images/2023/08/Bokeh_2_Isolation.png)](https://www.keheka.com/content/images/2023/08/Bokeh_2_Isolation.png)

Isolating the chosen kernel.

Crop in around the single bokeh kernel and tick _reformat_ in the Crop node to make a small image. A size of around 256x256 pixels is a good starting point, and you can use a Transform node to scale it up or down:

[![](https://www.keheka.com/content/images/2023/08/Bokeh_3_Crop.png)](https://www.keheka.com/content/images/2023/08/Bokeh_3_Crop.png)

Cropping in on the chosen kernel.

Use the result as a kernel for the Bokeh/ZDefocus node and apply the bokeh to your images:

[![](https://www.keheka.com/content/images/2023/08/Bokeh_4_Application.png)](https://www.keheka.com/content/images/2023/08/Bokeh_4_Application.png)

Another image defocused using our chosen kernel.

In the image above, even though the lights are much less bright, we’re still getting a bokeh with the same characteristics as in our source image.

By using a custom kernel like this, or by adjusting the bokeh settings in the Bokeh/ZDefocus nodes, you can create many different types of bokeh.

> [!important]
> 
> Keep the aspect ratio of the image in mind when you add bokeh. Anamorphic bokeh is elongated compared to spherical bokeh.

For a more advanced bokeh like the cats-eye bokeh, you can use a tool such as the [cfCatsEyeDefocus](https://www.chrisfryer.co.uk/post/cfcatseyedefocus?ref=keheka.com) gizmo.

### Field Curvature

Field Curvature is also known as Edge [of frame] Softness.

Curved lens surfaces don’t naturally project the scene onto a flat sensor, but rather they form the image on a domed surface.

Since the sensor of most cameras is flat, this will cause changes in focus across the image. Field curvature usually causes the edges of the frame to be out of focus while the centre is in focus, and vice-versa.

[![](https://www.keheka.com/content/images/2023/08/Simple_Field_Curvature_Diagram.jpg)](https://www.keheka.com/content/images/2023/08/Simple_Field_Curvature_Diagram.jpg)

Simple field curvature diagram.

The light rays converging at the centre of the image are sharp at one image plane (A), while the rays converging at the outer edges of the image are sharp at another (B).

Modern lenses contain many lens elements, and so a wavy field curvature may appear:

[![](https://www.keheka.com/content/images/2023/08/Wavy_Field_Curvature_Diagram.jpg)](https://www.keheka.com/content/images/2023/08/Wavy_Field_Curvature_Diagram.jpg)

Wavy field curvature diagram.

So the centre and edges can be sharp, while the mid-frame areas can be softer, and vice versa.

Field curvature also varies by distance. A lens that displays practically no visible field curvature at short range may exhibit field curvature when focused at infinity, for example.

### Practical Uses Of Field Curvature

Field curvature can be used creatively to draw attention to the central subject or to create a vintage or dream-like aesthetic.

By blurring the edges of the frame you naturally lead the viewer’s eyes to the sharp centre of the frame.

### How To Create Field Curvature In Nuke

The simplest way to create field curvature is to use an [iBlur](https://www.nukepedia.com/gizmos/filter/iblurfh?ref=keheka.com) node with a soft roto/inverted radial as the mask input.

A more advanced setup would be to have the iBlur driven by the actual lens distortion:

[![](https://www.keheka.com/content/images/2023/08/LensDistort_Field_Curvature_Setup.png)](https://www.keheka.com/content/images/2023/08/LensDistort_Field_Curvature_Setup.png)

Lens Distortion setup driving an iBlur.

[![](https://www.keheka.com/content/images/2023/08/LensDistort_Field_Curvature.png)](https://www.keheka.com/content/images/2023/08/LensDistort_Field_Curvature.png)

The top left of a Checkerboard with edge softness using the LensDistortion-driven technique. Notice the more distortion there is, the more softness there is.

First, we keep the image format but replace the red and green values with the X and Y pixel  
coordinates, in the Expression1 node:

[![](https://www.keheka.com/content/images/2023/08/LensDistortion_Driven_Edge_Blur_And_CA_Expression1.png)](https://www.keheka.com/content/images/2023/08/LensDistortion_Driven_Edge_Blur_And_CA_Expression1.png)

The expressions in the Expression1 node.

Then, we apply the LensDistortion node to these coordinates. This will distort the coordinates (now red and green values) and move them around according to the lens distortion.

Next, in the Expression2 node, we take the absolute value (i.e. a negative value becomes a positive value, and a positive value stays a positive value) of the difference between the original coordinates (x and y) and the distorted coordinates (r and g).

[![](https://www.keheka.com/content/images/2023/08/LensDistortion_Driven_Edge_Blur_And_CA_Expression2.png)](https://www.keheka.com/content/images/2023/08/LensDistortion_Driven_Edge_Blur_And_CA_Expression2.png)

The expressions in the Expression2 node.

That gives us the difference, i.e. how much the pixels have moved.

To convert this difference into a useful matte for the iBlur node, we max these new red and green values together and divide the result by 1000. This will give us alpha values above 0 but below 1 – in the case of a UHD image.

[![](https://www.keheka.com/content/images/2023/08/LensDistortion_Driven_Edge_Blur_And_CA_Expression3.png)](https://www.keheka.com/content/images/2023/08/LensDistortion_Driven_Edge_Blur_And_CA_Expression3.png)

The expressions in the Expression3 node.

> [!important]
> 
> Note that depending on your image format, you may have to increase or decrease the ‘1000’ value in the Expression3 node. For reference, this example was done on an UHD_4K 3840 x 2160 image.

What we’ve created in the alpha channel now is a soft matte that is getting stronger where there is more distortion in the image:

[![](https://www.keheka.com/content/images/2023/08/Field_Curvature_iBlur_Matte.png)](https://www.keheka.com/content/images/2023/08/Field_Curvature_iBlur_Matte.png)

Field curvature matte created for the iBlur node.

Lastly, connect this alpha to the mask input of the iBlur, and now you have field curvature driven by your lens distortion.

> [!important]
> 
> Note that this setup will only drive the area for where the iBlur should apply the defocus, and the relative strength. You will still have to adjust the maximum size value in the iBlur node to adjust the maximum defocus strength.

### Vignetting

Vignetting is a gradual darkening toward the corners and edges of the image.

We can divide vignettes into four types:

- Mechanical vignetting

- Optical vignetting

- Natural vignetting

- Pixel vignetting

Each type is caused by separate factors, and in total they form two different styles of vignetting. Let’s take a look:

### Mechanical Vignetting

Mechanical vignetting appears as a strong and usually abrupt darkening, typically only in the very corners of the frame.

This type of vignetting is caused by stacked or large filters, lens hoods, or other objects that physically block light from entering the camera lens, and/or from reaching the sensor.

Mechanical vignetting is normally cropped out from the image as it’s generally not desirable. It’s not possible to fully correct it with just grading because there is mostly just black pixel information in the affected areas (i.e., there is no detail to restore).

And so removing it requires manual cloning, patching, or warping (i.e. pushing the corners of the frame out).

[![](https://www.keheka.com/content/images/2023/08/Physical_Vignetting.jpg)](https://www.keheka.com/content/images/2023/08/Physical_Vignetting.jpg)

Mechanical vignetting.

> [!important]
> 
> In the image above you can see there is no image detail in the corners of the frame, only black pixels.

### Optical Vignetting

Optical vignetting is a softer, more subtle (less dark) vignetting.

It’s mostly visible in the corners of the frame, but can also creep into the edges of the frame and further into the image.

This type of vignetting is caused by rear elements inside of the camera lens being shaded by elements in front of them.

It can easily be corrected by grading up the image using a mask of the areas affected by the vignette.

[![](https://www.keheka.com/content/images/2023/08/Optical_Vignetting.jpeg)](https://www.keheka.com/content/images/2023/08/Optical_Vignetting.jpeg)

Optical vignetting.

> [!important]
> 
> In the image above you can see there _is_ image detail in the corners of the frame, it has just been darkened down.

### Natural Vignetting

Natural vignetting is also known as natural illumination falloff.

It is a physical phenomenon where the brightness of light is dependent on its angle in relation to the viewer.

In technical terms, the light falloff is proportional to the fourth power of the cosine of the angle at which the light strikes the sensor.

That just means the light entering the camera at more of an angle, for example at the outer edges of the lens, won’t appear as bright in the image as the light entering head on.

Modern camera lenses have greatly reduced this issue, but it’s often apparent in more compact or older cameras.

Natural vignetting looks similar to optical vignetting: A soft, gradual darkening towards the outer edges of the frame.

### Pixel Vignetting

Pixel vignetting is an effect that is unique to digital cameras, and it also looks similar to optical vignetting.

Digital sensors are flat and their pixels are all built the same way facing the same direction. The incoming light rays hit the pixels in the centre of the sensor head on, while they hit the pixels in the corners of the sensor at a slight angle.

The pixels in the corners therefore receive slightly less light compared to the ones in the  
centre, which leads to a vignetting effect.

### Practical Uses Of Vignetting

In the vast majority of cases, when we’re talking about vignetting in the context of a composite, we’re talking about optical vignetting.

That is, a gradual darkening around the edges of the frame.

Typically, there are three situations where you will work with vignettes:

- Grading or cropping out an unwanted optical or mechanical vignette from the plate.

- Matching the optical vignette in the plate – for CG renders, matte paintings, etc.

- Adding an optical vignette to the composite for artistic purposes. Either to  
    draw attention to a central subject/object, or to create a retro look.

On rare occasions you might get asked to warp out a mechanical vignette, but as mentioned above they are typically cropped out.

> [!important]
> 
> If you need to _remove_ vignetting from a shot by using a masked grade, keep in mind that brightening the image will increase the image noise/grain toward the corners and edges of the image. (Digitally brightening an image amplifies both the signal and the noise equally). And so you may need to replace the grain in those areas.

### How To Create A Vignette In Nuke

Creating a vignette in Nuke is very simple.

[![](https://www.keheka.com/content/images/2023/08/Vignette_Radial_Setup.png)](https://www.keheka.com/content/images/2023/08/Vignette_Radial_Setup.png)

Vignette setup using a Radial node.

Add a Grade node to your composite (**Grade1** in the setup above), and mask it by a Radial node.

Adjust the _area_ of the Radial node to fit your image. The alpha channel should look like a centred ellipse filling up the frame.

Add an Invert node to invert the alpha. This will change it from a centred ellipse to a corner/edges-of-frame matte.

Then, add a Blur node before the Invert node, and soften the alpha to taste (usually by a _size_ value in the hundreds).

Connect another Grade node (**Grade2** in the setup above) after the Blur node, set to affect the alpha channel, and change the _gamma_ and _whitepoint_ values to adjust the falloff and sharpness of the vignette mask.

Now, you can lower the _gain_ value in the **Grade1** node to add a vignette to your image.

If you want to brighten up the centre of the frame as well as darkening the corners/edges, mask another Grade node (**Grade3** in the setup above) by the non-inverted alpha you just created, and increase its _gain_ value.

> [!important]
> 
> There are also plenty of ready-made Vignette tools available on Nukepedia, for example [dg_Vignette](https://www.nukepedia.com/gizmos/colour/dg_vignette?ref=keheka.com).

# Optical Aberrations

An optical aberration is a lens defect which causes light to spread out instead of focusing to form a sharp image.

This can happen in different ways, and so optical flaws in the lens assembly lead to a variety of lens effects:

### Spherical Aberration

Spherical aberration is a physical effect that causes features in the image to go soft – but in a particular way:

### Soft Focus

A soft focused image caused by spherical aberration is not the same as a normal out-of-focus image, because sharp/defined outlines and details still remain visible:

[![](https://www.keheka.com/content/images/2023/08/Soft_Focus_Sharp.jpg)](https://www.keheka.com/content/images/2023/08/Soft_Focus_Sharp.jpg)

[![](https://www.keheka.com/content/images/2023/08/Soft_Focus.jpg)](https://www.keheka.com/content/images/2023/08/Soft_Focus.jpg)

[![](https://www.keheka.com/content/images/2023/08/Soft_Focus_Defocus.jpg)](https://www.keheka.com/content/images/2023/08/Soft_Focus_Defocus.jpg)

An image in sharp focus vs. the same image in soft focus vs. the same image out of focus.

> [!important]
> 
> The soft focus spherical aberration effect looks similar to what you can achieve with a diffusion filter attached to the lens; a sort of dreamy haze.

It happens because the light rays that strike the curved lens off-centre are refracted or reflected more or less than those that strike close to the centre:

[![](https://www.keheka.com/content/images/2023/08/Spherical_Aberration.jpg)](https://www.keheka.com/content/images/2023/08/Spherical_Aberration.jpg)

Light rays converging perfectly (top) versus light rays being affected by spherical aberration (bottom).

> [!important]
> 
> Modern lenses counteract spherical aberration, and you typically won’t see nearly as much of the effect as in the example above unless you’re using a special soft focus lens.

### Focus Shift

An interesting consequence of spherical aberration is focus shifting.

If you focus on an object in the scene using a wide aperture, and then  
stop down the lens (make the aperture narrower) for capturing the image, the image will go out of focus.

[![](https://www.keheka.com/content/images/2023/08/Focus_Shift.jpg)](https://www.keheka.com/content/images/2023/08/Focus_Shift.jpg)

Focus shift due to a change in aperture size.

When the lens is stopped down, the light rays that would travel through the outer edges of the lens are blocked and no longer contribute softness to the image.

The sharpest point of focus, i.e. the smallest circle of confusion, will be further back with a stopped down lens (small aperture) than with a wide aperture. And so, you may have to refocus after changing the aperture.

> [!important]
> 
> Focus shifting is more pronounced in ‘fast’ lenses, i.e. lenses with large apertures. For example f/1.0, f/1.2 and f/1.4. It becomes less and less noticeable the slower the lens, i.e. f/1.8 and smaller. It’s also more apparent when focusing on subjects close to the camera.

### Practical Uses Of Spherical Aberration

The spherical aberration effect creates a dreamy or hazy look.

It can be used to indicate that the character is dreaming, dying, or drugged, or that we are looking at a flashback of a memory.

[![](https://www.keheka.com/content/images/2023/08/Flashback.jpg)](https://www.keheka.com/content/images/2023/08/Flashback.jpg)

Hazy glow indicating a flashback.

It can also be used to recreate the romantic or glamorous soap opera look of the ‘80s and ‘90s.

[![](https://www.keheka.com/content/images/2023/08/Soap_Opera.png)](https://www.keheka.com/content/images/2023/08/Soap_Opera.png)

The soap opera ‘glow’.

### How To Create Spherical Aberration In Nuke

You can create the spherical aberration effect in the same way as previously covered in the _Diffusion_ section. It’s visually a gentle version of the same effect.

### Coma

Coma is an optical aberration where off-axis points of light appear comet-shaped in the image.

[![](https://www.keheka.com/content/images/2023/08/Coma_Example.jpg)](https://www.keheka.com/content/images/2023/08/Coma_Example.jpg)

Coma causing points of light to appear cone/comet shaped.

It happens because light rays from the edges of the frame get magnified differently depending on where they pass through the spherical surface of the lens:

[![](https://www.keheka.com/content/images/2023/08/Coma_Diagram.jpg)](https://www.keheka.com/content/images/2023/08/Coma_Diagram.jpg)

Off-axis light rays from a point of light converge at different focal points.

Here is an animation which better shows what’s happening:

> [!info] Coma Aberration  
> The contributions to coma aberration from different parts of the lens are shown.  
> [https://youtu.be/EXmaY2txEBo](https://youtu.be/EXmaY2txEBo)  

Coma aberration.

Coma is mostly corrected for in modern camera lenses, but it does show up in specific cases:

### Practical Uses Of Coma

If you're compositing a shot where a subject or object is viewed at a  
great magnification, such as a POV through a telescope or a microscope,  
the coma lens effect may show up.

So if you want to replicate astrophotography, for instance, consider applying coma to your images.

### How To Create Coma In Nuke

You can create Coma using a combination of a Transform node and a Blur node with modulus expressions applied to the _scale_ and _size_ values, and a TimeBlur node.

Essentially, you can fluctuate the scale and softness of points of lights over time, creating the cone shape. Then, mask the effect by a Radial node to only affect the periphery of the frame.

Here is a great tutorial on the modulus expression and the TimeBlur node, to explain how it works:

> [!info] Nuke | TimeBlur Tutorial | Subframes and Demos  
>  
> [https://vimeo.com/456737145](https://vimeo.com/456737145)  

TimeBlur tutorial.

The same method is used in this tool to recreate Coma:

[Defocus_Aberrations](https://www.nukepedia.com/gizmos/filter/defocus_aberrations?ref=keheka.com)

Select the group and hit Ctrl + Enter to enter it. Scroll down to the ‘Coma’ backdrop to see how the setup is put together.

### Halation

Halation is not a lens effect, specifically, but it is related and I thought it was worth including.

It's a warm glow in the image where the bright areas appear to softly bleed around the edges of dark areas, mostly in the red channel.

[![](https://www.keheka.com/content/images/2023/08/Halation.jpg)](https://www.keheka.com/content/images/2023/08/Halation.jpg)

Halation, here appearing as red fringing around the trees.

**Halation only occurs with film stock, and not in digital cameras.**

That's because film usually has three layers of colour-sensitive emulsion (for red, green, and blue) and an anti-halation backing _stacked on top of each other_.

[![](https://www.keheka.com/content/images/2023/08/Film_Strip.jpg)](https://www.keheka.com/content/images/2023/08/Film_Strip.jpg)

While a film strip appears to be a single layer, it's actually multiple layers stacked together.

So light moves _through_ each colour layer and exposes the film.

However, in a digital camera, the colour sensitive pixel components (red, green, and blue) are tightly packed side-by-side, and the light hits them at the same time.

[![](https://www.keheka.com/content/images/2023/08/Sensor.jpeg)](https://www.keheka.com/content/images/2023/08/Sensor.jpeg)

Colour-sensitive pixels are sat next to each other on a digital sensor.

So with film, a unique thing happens where some of the light that moves through the emulsion layers bounces back.

[![](https://www.keheka.com/content/images/2023/08/Halation_Effect.png)](https://www.keheka.com/content/images/2023/08/Halation_Effect.png)

Light bounces back, affecting the red-sensitive emulsion layer.

The light scatters a little bit and mostly hits the red emulsion layer of the film on the way back, causing a reddish halation in the image.

### Practical Uses Of Halation

Halation is mostly used to match or simulate shots filmed using a traditional film camera.

As mentioned above this particular effect doesn't physically happen in digital cameras. (You can replicate a similar look with a warm diffusion filter, though).

You can use it as one tool (along with film grain and other aberrations) to make your shot look more analog, adding a sense of nostalgia and warmth.

### How To Create Halation In Nuke

In its simplest form, halation is just a gentle red-tinted glow around bright, contrasted edges in the image.

You can use the same luma key-glow setup described in the _Diffusion_ section above, and just tint the glow red.

For more advanced options, here are two gizmos:

- [VirtualLens](https://www.nukepedia.com/gizmos/filter/virtuallens?ref=keheka.com)

- [Halation](https://www.nukepedia.com/gizmos/filter/halation?ref=keheka.com)

### Astigmatism

Astigmatism is an effect where light rays don’t focus to a single point, but rather multiple points along a line.

[![](https://www.keheka.com/content/images/2023/08/Astigmatism_Diagram.jpg)](https://www.keheka.com/content/images/2023/08/Astigmatism_Diagram.jpg)

Astigmatism diagram.

In the diagram above, you have the lens L, and the optical axis running along Q to Q1.

Light rays from an off-axis point P travel toward the edge of the image sensor.

The light rays entering the lens in the Sagittal plane are focused at a different place (S1) than the light rays entering in the (perpendicular) Tangential plane (which focus at T1). A point of light therefore becomes spread out into a line to some degree.

Astigmatism doesn’t affect the centre of the image (unless the lens elements are misaligned). It becomes more pronounced further out from the centre of the image and is most severe at the edges and corners of the frame.

[![](https://www.keheka.com/content/images/2023/08/Astigmatism.jpg)](https://www.keheka.com/content/images/2023/08/Astigmatism.jpg)

Astigmatism, here appearing as double edge artefacts on the screen left number plate.

Astigmatism can also cause bright lights to flare out in diagonal streaks:

[![](https://www.keheka.com/content/images/2023/08/Streaks1.jpg)](https://www.keheka.com/content/images/2023/08/Streaks1.jpg)

[![](https://www.keheka.com/content/images/2023/08/Streaks2.jpg)](https://www.keheka.com/content/images/2023/08/Streaks2.jpg)

[![](https://www.keheka.com/content/images/2023/08/Streaks3.jpg)](https://www.keheka.com/content/images/2023/08/Streaks3.jpg)

Streaking lights caused by astigmatism.

### Practical Uses Of Astigmatism

Astigmatism can for example be used as a visual effect to indicate that someone is dazed, drowsy, or drugged in a POV shot.

### How To Create Astigmatism In Nuke

One way to create astigmatism is to use a soft line as the filter  
input for a Convolve node, and rotate the line diagonally. Please take a look inside the [VirtualLens](https://www.nukepedia.com/gizmos/filter/virtuallens?ref=keheka.com) gizmo for how to achieve this.

Another method is to perform a bit of image manipulation in combination with a glow, like I have shown in this tutorial: [Creating Custom Lens Effects By Pre-Treating Your Images In Nuke](https://www.keheka.com/creating-custom-lens-effects-by-pre-treating-your-images-in-nuke/).

### Chromatic Aberration

Chromatic aberration is also known as colour fringing.

When the different wavelengths of light do not converge at the same focal point, colour fringes appear around high-contrast edges in the image.

Like we saw in the _Circle Of Confusion_ section, a lens is unable to perfectly bring every wavelength of light onto the same focal plane, i.e. the sensor plane.

But it may _also_ focus the wavelengths at different _positions_ on the sensor plane.

And so we therefore have **two different types of chromatic aberration**, longitudinal (also known as axial) aberration, and lateral (also known as transverse) aberration.

Before we dive into each of them, let’s look at what an imaginary perfect lens would do:

[![](https://www.keheka.com/content/images/2023/08/No_Chromatic_Aberration.png)](https://www.keheka.com/content/images/2023/08/No_Chromatic_Aberration.png)

A perfect lens would converge all of the different wavelengths at the same focal point, i.e. exactly on the sensor.

With modern, complex lenses we can get close to this, but not all the way there. And so we get chromatic aberrations:

### Longitudinal/Axial Chromatic Aberration

This aberration causes the wavelengths of the light to spread along the optical axis.

Only some, often the green, wavelengths are in sharp focus, while the others converge before or after the sensor plane.

[![](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration_Diagram.png)](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration_Diagram.png)

With longitudinal chromatic aberration, the wavelengths do not all converge on the sensor plane.

[![](https://www.keheka.com/content/images/2023/08/Axial_Chromatic_Aberration.png)](https://www.keheka.com/content/images/2023/08/Axial_Chromatic_Aberration.png)

Longitudinal chromatic aberration affects the whole image, not just the outer edges of the frame.

[![](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration.jpg)](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration.jpg)

Longitudinal chromatic aberration can appear as different coloured haloes at different depths or parts of the image. Here, it shows up as a purple halo around the edges close to the camera, and as a green halo around the edges further away.

> [!important]
> 
> Although longitudinal chromatic aberration typically occurs uniformly around all edges, it may not appear equally in all directions. It depends on the colour and brightness of that particular edge. This can often cause it to be mistaken for lateral chromatic aberration.

### Lateral/Transverse Chromatic Aberration

This aberration causes the wavelengths of the light to spread along the sensor plane.

All of the wavelengths are in focus, but they each converge at different focal points on the sensor plane.

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_Diagram.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_Diagram.png)

With lateral chromatic aberration, the wavelengths _do_ converge on the sensor plane, but at different positions.

[![](https://www.keheka.com/content/images/2023/08/Lateral_Chromatic_Aberration.png)](https://www.keheka.com/content/images/2023/08/Lateral_Chromatic_Aberration.png)

Lateral chromatic aberration affects the outer edges of the image more than the centre.

[![](https://www.keheka.com/content/images/2023/08/Lateral_CA.png)](https://www.keheka.com/content/images/2023/08/Lateral_CA.png)

Lateral chromatic aberration shifts colours along contrasted edges in the image.

### Dynamic Duo

Both lateral and longitudinal chromatic aberrations can occur **at the same time** in an image.

And so you may see all sorts of fringe colours smear and blend. The most common combinations are:

- Red-cyan

- Purple-green

- Yellow-blue

It all depends on the camera, lens, and the conditions.

### Practical Uses Of Chromatic Aberration

Chromatic aberration is typically applied to CG renders to match  
what’s happening in the scan, or to full-CG shots for added realism.

But this lens effect can also be useful in many other ways, for example:

[![](https://www.keheka.com/content/images/2023/08/CA_Motion_Graphics.jpg)](https://www.keheka.com/content/images/2023/08/CA_Motion_Graphics.jpg)

Chromatic aberration used in motion graphics.

[![](https://www.keheka.com/content/images/2023/08/CA_Logo.png)](https://www.keheka.com/content/images/2023/08/CA_Logo.png)

Chromatic aberration used in a logo.

[![](https://www.keheka.com/content/images/2023/08/CA_Glitch.jpg)](https://www.keheka.com/content/images/2023/08/CA_Glitch.jpg)

Chromatic aberration used in a glitch effect.

### How To Create Chromatic Aberrations In Nuke

The first port of call is to analyse your footage:

What’s happening in the scan? Take a look around the edges of the frame, and in high-contrast areas.

Keep in mind that there may be a combination of both types of chromatic aberration, and/or that the aberrations could be skewed more towards one side of an object than the other.

Once you have a good idea of what you need to match, you can start building your chromatic aberration.

The simplest way of creating a **lateral chromatic aberration** is by using a Transform node.

It’s quite straightforward: just scale up your image by a low value such as 1.0015, and make the scaling only affect the channels you want using a Dissolve node.

> [!important]
> 
> Make sure the _center_ value in the Transform node is at the centre of your image **before** you scale it up. A quick way to do that is to connect the Transform node to your image, and right-clicking on its properties and choosing _Set knobs to default_.

For example, if your footage has lateral aberration with red on the outer edge and cyan on the inner edge, dissolve just the **red** channel of the scaled up image with the original image. (In the Dissolve node, set the _which_ knob to 1 for the full effect, or mix it back as needed).

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_Transform_Setup.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_Transform_Setup.png)

Setup for creating lateral chromatic aberration by using a Transform node and scaling the red channel.

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_Transform.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_Transform.png)

The top left of a Checkerboard with lateral chromatic aberration added using the Transform technique.

If you zoom in on the picture above, you can see that this method visually affects the outer edges of the frame more than the centre of the frame, like we would expect.

> [!important]
> 
> Should it be the other way around, and cyan is on the outer edge, change the Dissolve node to affect the **green** and **blue** channels instead of the red channel. Doing it that way instead of scaling down the red channel helps avoid crop issues at the edges of the frame. So try to keep the scale value **positive**.

If your footage has more of a blue-yellow aberration, choose the blue channel in the Dissolve node (or the red and green channels). And if the aberration is purple-green, choose the green channel in the Dissolve node (or the red and blue channels).

Instead of using a _Transform_ node, a more accurate way would be to use a **LensDistortion** node in the same setup:

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_LensDistort_Setup.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_LensDistort_Setup.png)

Setup for creating lateral chromatic aberration by using a LensDistortion node and scaling the red channel.

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_LensDistort.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_LensDistort.png)

The top left of a Checkerboard with lateral chromatic aberration added using the LensDistortion technique.

If you zoom in on the picture above, you can see that this method also correctly affects the outer edges of the frame more than the centre of the frame. However, the transition is more natural than with the Transform method, and the centre is even less affected.

For a soft and quite fast-rendering chromatic aberration, you can use one or more **GodRays** nodes:

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_GodRays_Setup.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_GodRays_Setup.png)

Setup for creating lateral chromatic aberration by using a GodRays node and scaling the red channel.

[![](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_GodRays.png)](https://www.keheka.com/content/images/2023/08/Lateral_Aberration_GodRays.png)

The top left of a Checkerboard with lateral chromatic aberration added using the GodRays technique.

> [!important]
> 
> Just remember to manually set the _center_ knob of the GodRays node to the centre of your image **before** you scale it, as the _Set knobs to default_ trick doesn’t work with this node.

Creating a **longitudinal chromatic aberration** is also quite easy:

Simply connect a Blur node to your comp and gently blur the red and blue channels. Then, mask the Blur node by an [edge matte](https://www.keheka.com/perimeter-advanced-edge-mattes/) of your CG, and/or a luma key of the highlights in the image.

[![](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration_Blur_Setup.png)](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration_Blur_Setup.png)

Setup for creating longitudinal chromatic aberration using a Blur node.  
Remember to mask it by a luma key of the highlights in the image or an edge matte created from the alpha of your CG render.

[![](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration_Blur.png)](https://www.keheka.com/content/images/2023/08/Longitudinal_Aberration_Blur.png)

The top left of a Checkerboard with longitudinal chromatic aberration added using the Blur technique.

Here are some great chromatic aberration tools:

- [apChroma](https://www.nukepedia.com/gizmos/filter/apchroma?ref=keheka.com)

- [Chromatik](https://github.com/SpinVFX/spin_nuke_gizmos?ref=keheka.com)

- [fxT_chromaticAberration](https://www.nukepedia.com/gizmos/filter/chromaticabberation?ref=keheka.com)

# Motion Blur

Motion blur is not a lens effect per se (although lens movement does impact it).

However, it is a common part of compositing and so I thought it would be useful for you to know where in the chain it sits.

> [!important]
> 
> If you’re interested in going further down the rabbit hole, I have covered motion blur in detail in [Motion Blur Masterclass For Compositors](https://www.keheka.com/motion-blur-masterclass-for-compositors/).

Motion blur is first and foremost dependent on the shutter. Mechanical shutters typically sit right in front of the sensor, and electronic shutters are just switching the digital sensor on and off to make the exposure.

In both cases, the shutter action essentially happens right before the light hits the sensor. And so motion blur appears very late in the chain of lens effects applied to your CG renders.

I’ve seen people debating the order in which to apply defocus and motion blur to CG renders (assuming they were rendered without depth of field and motion blur – and instead include depth and motion vector passes), however that one is quite easy to settle because **motion blur will smear the bokeh**.

If you look at long exposure night time photography, for example, you can see long trails of motion blurred light that would otherwise have shown up as bokeh:

[![](https://www.keheka.com/content/images/2023/08/Smeared_Bokeh.png)](https://www.keheka.com/content/images/2023/08/Smeared_Bokeh.png)

Smeared bokeh due to motion blur from long exposure photography. Notice the static red lights bottom left showing up as bokeh, while the moving red lights smear the bokeh along their path.

So motion blur should technically be applied after the defocus. (However, unless there is a strong bokeh effect you probably won’t see much difference).

> [!important]
> 
> Motion blur does not smear any lens contaminations stuck on the lens, because they follow the motion of the lens. So only apply motion blur to your CG, and not to any lens dirt. Exceptions include rain drops or splatter running down the lens – i.e. lens contaminations moving in relation to the lens – they would receive motion blur.

# Bonus Tip

Unless going for a specific look, photographers try to minimise aberrations.

As you have seen, there are a _lot_ of effects we compositors may have to match during compositing in order to produce a photographic composite. And so if you are out shooting for a personal project, for example, you can save yourself some work simply by remembering that **most lens effects are caused by optical issues at the outer edges of the lens**.

Which means that most aberrations are prevalent at low f/numbers (wide apertures). So the easiest way to minimise aberrations is to stop the lens to a higher f/number (smaller aperture).

Closing the aperture one or two stops down from wide-open will reduce the total light collected by the camera, but it will also improve sharpness and reduce aberrations.

Looking again at the diagram shown previously, you can see that a smaller aperture blocks out problematic light rays at the periphery of the lens:

[![](https://www.keheka.com/content/images/2023/08/Focus_Shift-1.jpg)](https://www.keheka.com/content/images/2023/08/Focus_Shift-1.jpg)

A stopped down aperture blocks out problematic light rays.

I hope you found this **Companions Exclusive Guide** useful. For more Nuke tips & tricks, see [Nuke](https://www.keheka.com/tag/nuke/).